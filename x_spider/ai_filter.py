from openai import OpenAI
import os
import json
import re
from typing import List, Dict, Any
from datetime import datetime

client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    base_url=os.environ.get("OPENAI_BASE_URL"),
)
base_model = os.environ.get("OPENAI_BASE_MODEL")

base_system_prompt = ""

with open('./prompts/x_signal.txt', 'r', encoding='utf-8') as file:
    base_system_prompt = file.read()


def call_llm_api(prompt):
    """è°ƒç”¨ OpenAI API è¿›è¡Œæ¨æ–‡åˆ†æ"""
    try:
        # æ£€æŸ¥å¿…è¦çš„é…ç½®
        if not base_model:
            return "APIé…ç½®é”™è¯¯: OPENAI_BASE_MODEL ç¯å¢ƒå˜é‡æœªè®¾ç½®"
        
        if not client.api_key:
            return "APIé…ç½®é”™è¯¯: OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®"
        
        print(f"ä½¿ç”¨æ¨¡å‹: {base_model}")
        print(f"API Base URL: {client.base_url}")
        
        stream = client.chat.completions.create(
            model=base_model,
            max_tokens=20000,
            stream=True,
            messages=[
                {"role": "system", "content": base_system_prompt},
                {"role": "user", "content": prompt},
            ],
        )
        
        # æ”¶é›†æµå¼å“åº”çš„å†…å®¹
        content = ""
        for chunk in stream:
            # æ£€æŸ¥æ˜¯å¦æœ‰choicesä¸”ä¸ä¸ºç©º
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if delta.content is not None:
                    chunk_content = delta.content
                    content += chunk_content

        return content
    except Exception as e:
        print(f"APIè°ƒç”¨è¯¦ç»†é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"


# ä»æ•°æ®åº“è¯»å–æœ€æ–°çš„æ¨æ–‡æ•°æ®ï¼Œæ”¯æŒè¿‡æ»¤å·²åˆ†æçš„å†…å®¹
def get_latest_x_data(limit: int = 20, skip_analyzed: bool = True) -> List[Dict[str, Any]]:
    """
    è·å–æœ€æ–°çš„æ¨æ–‡æ•°æ®
    Args:
        limit: è¦è·å–çš„æ•°æ®æ¡æ•°
        skip_analyzed: æ˜¯å¦è·³è¿‡å·²åˆ†æçš„å†…å®¹
    Returns:
        æ¨æ–‡æ•°æ®åˆ—è¡¨
    """
    from db_utils import get_db_connection
    import psycopg2.extras
    
    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:
            if skip_analyzed:
                # è¿‡æ»¤å·²åˆ†æçš„å†…å®¹ï¼ˆdataä¸­ä¸åŒ…å«ai_resultå­—æ®µï¼‰
                query = """
                    SELECT id, x_id, item_type, data, username, user_id, user_link, created_at 
                    FROM t_x 
                    WHERE NOT (data ? 'ai_result')
                    ORDER BY created_at DESC 
                    LIMIT %s
                """
            else:
                # è·å–æ‰€æœ‰æ•°æ®
                query = """
                    SELECT id, x_id, item_type, data, username, user_id, user_link, created_at 
                    FROM t_x 
                    ORDER BY created_at DESC 
                    LIMIT %s
                """
            
            cur.execute(query, (limit,))
            rows = cur.fetchall()
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            results = []
            for row in rows:
                result = dict(row)
                # ç¡®ä¿ created_at æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                if result['created_at']:
                    result['created_at'] = result['created_at'].isoformat()
                results.append(result)
            
            return results
    
    except Exception as e:
        print(f"Error fetching X data: {e}")
        raise
    finally:
        if conn:
            conn.close()


# è§£ææ¨æ–‡å†…å®¹ï¼Œå‚è€ƒå‰ç«¯æ¸²æŸ“é€»è¾‘
def extract_tweet_content(data: Dict[str, Any]) -> str:
    """ä»æ¨æ–‡æ•°æ®ä¸­æå–æ­£æ–‡å†…å®¹"""
    content_parts = []
    
    # å¤„ç†å•ä¸ªæ¨æ–‡æ•°æ®
    def process_single_tweet(tweet_data: Dict[str, Any]) -> str:
        full_text = tweet_data.get('full_text', '')
        if not full_text:
            return ''
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        full_text = re.sub(r'\s+', ' ', full_text.strip())
        
        # å¤„ç†è½¬æ¨å’Œå¼•ç”¨çš„åˆ†å‰²
        # åœ¨æ­£æ–‡ä¸­é‡åˆ° "RT @" æˆ– "quoted From @"ï¼ˆéå¼€å¤´ï¼‰æ’å…¥åˆ†å‰²æ ‡è®°
        separator_regex = r'(RT\s@|quoted From\s@)'
        first_non_ws = next((i for i, c in enumerate(full_text) if not c.isspace()), 0)
        
        parts = []
        for match in re.finditer(separator_regex, full_text, re.IGNORECASE):
            idx = match.start()
            if idx > first_non_ws:  # ä¸åœ¨å¼€å¤´
                before = full_text[:idx].strip()
                after = full_text[idx:].strip()
                if before:
                    parts.append(before)
                if after:
                    prefix = 'è½¬æ¨: ' if after.lower().startswith('rt @') else 'å¼•ç”¨: '
                    parts.append(prefix + after)
                return ' | '.join(parts)
        
        return full_text
    
    # æ ¹æ®æ•°æ®ç±»å‹å¤„ç†
    if isinstance(data, list):
        # profile-conversation ç±»å‹
        for item in data:
            if isinstance(item, dict) and 'data' in item:
                tweet_content = process_single_tweet(item['data'])
                if tweet_content:
                    content_parts.append(tweet_content)
    elif isinstance(data, dict):
        # å•ä¸ªæ¨æ–‡ç±»å‹
        tweet_content = process_single_tweet(data)
        if tweet_content:
            content_parts.append(tweet_content)
    
    return ' | '.join(content_parts)


# è°ƒç”¨LLMåˆ†ææ¨æ–‡
def analyze_x_data(x_data: List[Dict[str, Any]]) -> str:
    """åˆ†ææ¨æ–‡æ•°æ®å¹¶è¿”å›LLMç»“æœ"""
    # æ„å»ºæ¨æ–‡å†…å®¹å­—ç¬¦ä¸²
    tweet_contents = []
    
    for item in x_data:
        x_id = item.get('x_id', '')
        username = item.get('username', '')
        user_id = item.get('user_id', '')
        
        # è§£ææ¨æ–‡å†…å®¹
        try:
            data = item.get('data')
            if isinstance(data, str):
                data = json.loads(data)
            
            content = extract_tweet_content(data)
            if content:
                tweet_info = f"x_id: {x_id}\nç”¨æˆ·: @{username} ({user_id})\nå†…å®¹: {content}\n"
                tweet_contents.append(tweet_info)
        except (json.JSONDecodeError, Exception) as e:
            print(f"Error parsing tweet data for {x_id}: {e}")
            continue
    
    if not tweet_contents:
        return '[]'  # è¿”å›ç©ºçš„JSONæ•°ç»„
    
    # æ„å»ºæ¨æ–‡å†…å®¹ä½œä¸ºç”¨æˆ·è¾“å…¥
    tweets_text = "\n" + "="*50 + "\n".join(tweet_contents)
    
    result = call_llm_api(tweets_text)
    return result


def parse_llm_result(result: str) -> List[Dict[str, Any]]:
    """è§£æLLMè¿”å›çš„JSONç»“æœ"""
    if not result or not result.strip():
        return []
    
    try:
        # å°è¯•ç›´æ¥è§£æJSON
        parsed = json.loads(result.strip())
        
        # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼Œè½¬æ¢ä¸ºæ•°ç»„
        if isinstance(parsed, dict):
            parsed = [parsed]
        
        # éªŒè¯æ¯ä¸ªç»“æœçš„æ ¼å¼
        valid_results = []
        for item in parsed:
            if isinstance(item, dict) and 'x_id' in item:
                # æ¸…ç†å’ŒéªŒè¯æ•°æ®
                clean_item = {
                    'x_id': str(item['x_id']),
                    'summary': str(item.get('summary', '')).strip(),
                    'highlight_label': item.get('highlight_label', []),
                    "model": base_model
                }
                
                # ç¡®ä¿ highlight_label æ˜¯æ•°ç»„
                if not isinstance(clean_item['highlight_label'], list):
                    clean_item['highlight_label'] = []
                
                # æ¸…ç† highlight_label ä¸­çš„å…ƒç´ 
                clean_item['highlight_label'] = [
                    str(label).strip() for label in clean_item['highlight_label'] 
                    if str(label).strip()
                ]
                
                if clean_item['summary']:  # åªæœ‰æœ‰summaryçš„æ‰ä¿ç•™
                    valid_results.append(clean_item)
        
        return valid_results
        
    except json.JSONDecodeError:
        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æå–JSONå—
        print(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–JSONå—...")
        
        # ä½¿ç”¨æ­£åˆ™æå–JSONå—
        json_pattern = r'\[\s*\{[^}]*\}(?:\s*,\s*\{[^}]*\})*\s*\]|\{[^}]*\}'
        matches = re.findall(json_pattern, result, re.DOTALL)
        
        for match in matches:
            try:
                parsed = json.loads(match)
                if isinstance(parsed, dict):
                    parsed = [parsed]
                return parse_llm_result(json.dumps(parsed))  # é€’å½’è°ƒç”¨
            except json.JSONDecodeError:
                continue
        
        print(f"Unable to parse LLM result: {result[:200]}...")
        return []
    
    except Exception as e:
        print(f"Error parsing LLM result: {e}")
        return []

def save_llm_result(ai_results: List[Dict[str, Any]], analyzed_x_ids: List[str]) -> None:
    """å°†AIåˆ†æç»“æœä¿å­˜åˆ°æ•°æ®åº“ï¼Œå¹¶æ ‡è®°æ‰€æœ‰å·²åˆ†æçš„æ¨æ–‡"""
    from db_utils import get_db_connection
    
    conn = None
    try:
        conn = get_db_connection()
        updated_count = 0
        
        with conn.cursor() as cur:
            # å…ˆå¤„ç†æœ‰AIåˆ†æç»“æœçš„æ¨æ–‡
            for result in ai_results:
                x_id = result['x_id']
                
                # è·å–å½“å‰è®°å½•çš„dataå­—æ®µ
                cur.execute("SELECT data FROM t_x WHERE x_id = %s", (x_id,))
                row = cur.fetchone()
                
                if not row:
                    print(f"Warning: No record found for x_id: {x_id}")
                    continue
                
                current_data = row[0]
                if isinstance(current_data, str):
                    current_data = json.loads(current_data)
                
                # åœ¨dataä¸­æ·»åŠ ai_resultå­—æ®µï¼ˆé‡è¦ä¿¡å·ï¼‰
                current_data['ai_result'] = {
                    'summary': result['summary'],
                    'highlight_label': result['highlight_label'],
                    'analyzed_at': datetime.now().isoformat(),
                    'is_important': True
                }
                
                # æ›´æ–°æ•°æ®åº“
                cur.execute(
                    "UPDATE t_x SET data = %s WHERE x_id = %s",
                    (json.dumps(current_data), x_id)
                )
                updated_count += 1
            
            # å¤„ç†å·²åˆ†æä½†æ— é‡è¦ä¿¡å·çš„æ¨æ–‡
            result_x_ids = {result['x_id'] for result in ai_results}
            no_signal_x_ids = [x_id for x_id in analyzed_x_ids if x_id not in result_x_ids]
            
            for x_id in no_signal_x_ids:
                # è·å–å½“å‰è®°å½•çš„dataå­—æ®µ
                cur.execute("SELECT data FROM t_x WHERE x_id = %s", (x_id,))
                row = cur.fetchone()
                
                if not row:
                    continue
                
                current_data = row[0]
                if isinstance(current_data, str):
                    current_data = json.loads(current_data)
                
                # æ ‡è®°ä¸ºå·²åˆ†æä½†æ— é‡è¦ä¿¡å·
                current_data['ai_result'] = {
                    'analyzed_at': datetime.now().isoformat(),
                    'is_important': False,
                    'summary': None,
                    'highlight_label': []
                }
                
                # æ›´æ–°æ•°æ®åº“
                cur.execute(
                    "UPDATE t_x SET data = %s WHERE x_id = %s",
                    (json.dumps(current_data), x_id)
                )
                updated_count += 1
        
        conn.commit()
        print(f"Successfully updated {updated_count} records:")
        print(f"  - {len(ai_results)} records with important signals")
        print(f"  - {len(no_signal_x_ids)} records marked as analyzed (no important signals)")
        
    except Exception as e:
        print(f"Error saving AI results: {e}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            conn.close()


def main():
    print(f"ğŸš€ å¼€å§‹è·å–æ¨æ–‡æ•°æ®...")
    x_data = get_latest_x_data(limit=20, skip_analyzed=False)
        
    if not x_data:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ¨æ–‡æ•°æ®")
        return
        
    print(f"ğŸ“Š æ‰¾åˆ° {len(x_data)} æ¡éœ€è¦åˆ†æçš„æ¨æ–‡")
    
    # è®°å½•æ‰€æœ‰è¦åˆ†æçš„æ¨æ–‡ID
    analyzed_x_ids = [item['x_id'] for item in x_data]
        
    # AIåˆ†æ
    print(f"ğŸ¤– å¼€å§‹AIåˆ†æ...")
    llm_result = analyze_x_data(x_data)
        
    if not llm_result or llm_result.strip() == '[]':
        print("âš ï¸ AIåˆ†ææœªè¿”å›æœ‰æ•ˆç»“æœ")
        # å³ä½¿æ²¡æœ‰ç»“æœï¼Œä¹Ÿè¦æ ‡è®°è¿™äº›æ¨æ–‡å·²ç»è¢«åˆ†æè¿‡
        print("ğŸ“ æ ‡è®°æ¨æ–‡ä¸ºå·²åˆ†æï¼ˆæ— é‡è¦ä¿¡å·ï¼‰...")
        save_llm_result([], analyzed_x_ids)
        return
        
    # è§£æç»“æœ
    print(f"ğŸ” è§£æAIè¿”å›ç»“æœ...")
    parsed_results = parse_llm_result(llm_result)
        
    if not parsed_results:
        print(f"âš ï¸ æœªèƒ½è§£æå‡ºAIç»“æœ")
        print(f"AIåŸå§‹è¿”å›: {llm_result[:500]}...")
        # æ ‡è®°ä¸ºå·²åˆ†æä½†æ— æœ‰æ•ˆç»“æœ
        print("ğŸ“ æ ‡è®°æ¨æ–‡ä¸ºå·²åˆ†æï¼ˆè§£æå¤±è´¥ï¼‰...")
        save_llm_result([], analyzed_x_ids)
        return
        
    print(f"ğŸ‰ æˆåŠŸè§£æå‡º {len(parsed_results)} æ¡é«˜ä»·å€¼ä¿¡å·")
        
    # æ˜¾ç¤ºç»“æœé¢„è§ˆ
    for result in parsed_results[:5]:
        print(f"  â€¢ {result['x_id']}: {result['summary']} [{', '.join(result['highlight_label'])}]")
    
    # ä¿å­˜ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜AIåˆ†æç»“æœ...")
    save_llm_result(parsed_results, analyzed_x_ids)
    
    print(f"âœ… å®Œæˆï¼å…±å¤„ç†äº† {len(analyzed_x_ids)} æ¡æ¨æ–‡ï¼Œå…¶ä¸­ {len(parsed_results)} æ¡ä¸ºé«˜ä»·å€¼ä¿¡å·")


if __name__ == "__main__":
    main()